## 投票分类器

预测器尽可能独立(不是同种预测器，预测算法不同)，voting法的真实效果才会好

硬投票：投票最多的类作为类别

软投票：各个类平均概率最好的类作为类别

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 单个模型
rnd_forest_clf = RandomForestClassifier()
log_reg = LogisticRegression()
svm_clf = SVC()

print('hard:')  # 硬投票
vt_clf_hard = VotingClassifier(
    estimators=[
        ('rnd', rnd_forest_clf),
        ('lg', log_reg),
        ('svm', svm_clf)
    ],
    voting='hard'
)
for clf in (rnd_forest_clf, log_reg, svm_clf, vt_clf_hard):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(f'{clf.__class__.__name__}: {accuracy_score(y_test, y_pred)}')

print('soft')  # 软投票
svm_clf = SVC(probability=True)  # 只要保证每个分类器有概率的计算即可
vt_clf_hard = VotingClassifier(
    estimators=[
        ('rnd', rnd_forest_clf),
        ('lg', log_reg),
        ('svm', svm_clf)
    ],
    voting='soft'
)
for clf in (rnd_forest_clf, log_reg, svm_clf, vt_clf_hard):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(f'{clf.__class__.__name__}: {accuracy_score(y_test, y_pred)}')
```

## bagging和pasting

部署多个相同分类器，只对一种算法进行训练。

bagging和pasting的区别是相对单个分类器来说的：

![image-20220831115529968](C:\Users\Mr.K\AppData\Roaming\Typora\typora-user-images\image-20220831115529968.png)

包外评估：若采用bagging方法，学习完成后存在约37%的训练样例未被采样，用这部分来评估分类效果

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=500,
    max_samples=100,
    bootstrap=True,
    oob_score=True
)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
print(f'accuracy: {accuracy_score(y_test, y_pred)}')
print(f'oob_score: {bag_clf.oob_score_}')
```

```python
# 超参数
n_estimators: 采用多少个分类器集成
max_samples: 抽样的最大样本数
bootstrap：是否放回(bagging还是pasting)
oob_score: 是否采用oob评估

# 属性
bag_clf.oob_score_: 包外评估准确度
bag_clf.oob_decision_function_: oob决策函数
```

## 随机补丁与随机子空间法？？

bagging和pasting：对实例进行抽样

随即补丁：对实例和特征都进行抽样

随机子空间：保留实例对特征进行抽样

## 随机森林

### 标准随机森林

可以用bagging法实现，但专门的模块更优化

特征选取有随机性：用偏差换取方差

```python
rom sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
```

### 方法、属性和超参数

同时具有决策树和bagging法的属性参数。包括特征重要性

```python
# 超参数
```

## 极端随机森林

对于确定特征的阈值，也是用随机阈值。

```python
from sklearn.ensemble import ExtraTreesClassifier
```

其他api基本上都相同

## adaboost

```python
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), 
    n_estimators=200,
    algorithm="SAMME.R", 
    learning_rate=0.5, 
    random_state=42
)
ada_clf.fit(X_train, y_train)
```

## GBRT

p168 根据原理实现GBRT

基本用法（超参数和决策树一样）：

```python
from sklearn.ensemble import GradientBoostingRegressor

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
# learning_rate代表每棵树的贡献率，贡献率越低需要越多的树
gbrt.fit(X, y)
```

用staged_predict()方法寻找最优的树的数量：

```python
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
gbrt.fit(X_train, y_train)

errors = [mean_squared_error(y_val, y_pred)
          for y_pred in gbrt.staged_predict(X_val)]
bst_n_estimators = np.argmin(errors) + 1

gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)
gbrt_best.fit(X_train, y_train)
```

逐步训练，在损失不再减小时停止增加树终止训练：

```python
gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)  # 使用warm_start参数

min_val_error = float("inf")
error_going_up = 0
for n_estimators in range(1, 120):
    gbrt.n_estimators = n_estimators
    gbrt.fit(X_train, y_train)
    y_pred = gbrt.predict(X_val)
    val_error = mean_squared_error(y_val, y_pred)
    if val_error < min_val_error:
        min_val_error = val_error
        error_going_up = 0
    else:
        error_going_up += 1
        if error_going_up == 5:
            break  # early stopping
```

其他的一些超参数：

```python
subsample: 每棵树可使用的训练集的比例
loss：损失函数
```

