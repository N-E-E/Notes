## MNIST数据集

### 数据集统一操作

- 数据集是一个字典对象

- 常用的字典键：

  DESCR：数据集的介绍

  data：数据，应该是dataframe类型

  feature_names: 每一列的特征名

  target：数据标签

  ```python
  mnist = fetch_openml('mnist_784', version=1)
  print(mnist['DESCR'])
  ```

## 二元分类

### SGD分类器

```python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)
sgd_clf.fit(X_train, y_train_5)
sgd_clf.predict([some_digit])

from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

```

原理：

见书p93

属性：

```python
sgd.decision_function([digit])  # 根据决策函数返回函数值

```

### 随机森林分类器

```python
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,
                                    method="predict_proba")
```

原理：预测的是概率而不是决策函数值

## 性能评估

### 混淆矩阵

要记忆精度和召回率在矩阵中的体现

```python
# 计算混淆矩阵
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)  # 通过k折验证得到每个数据的预测值
print(y_train_pred)
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_train_5, y_train_pred))
```

### 精度,召回率和f1-score

- 个人理解：

  精度：有正有负的一堆类数据中能挑出真正类的能力

  召回率：一堆正类数据中把数据划为正类的能力

- 两个指标都高时f1才会高

- ```python
  # 计算精度和召回率及f1score
  from sklearn.metrics import precision_score, recall_score, f1_score
  print(precision_score(y_train_5, y_train_pred))
  print(recall_score(y_train_5, y_train_pred))
  print(f1_score(y_train_5, y_train_pred))
  ```

### 精度与召回率的权衡

这方面需要考虑分类器的原理进行分析

绘制曲线显示精度和召回率(要理解为什么精度会有起伏)：

```python
y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,
                             method="decision_function")

from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)

# 关于阈值的图
def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)
    plt.legend(loc="center right", fontsize=16) # Not shown in the book
    plt.xlabel("Threshold", fontsize=16)        # Not shown
    plt.grid(True)                              # Not shown
    plt.axis([-50000, 50000, 0, 1])             # Not shown

recall_90_precision = recalls[np.argmax(precisions >= 0.90)]
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]

plt.figure(figsize=(8, 4))                                                                  # Not shown
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], "r:")                 # Not shown
plt.plot([-50000, threshold_90_precision], [0.9, 0.9], "r:")                                # Not shown
plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], "r:")# Not shown
plt.plot([threshold_90_precision], [0.9], "ro")                                             # Not shown
plt.plot([threshold_90_precision], [recall_90_precision], "ro")                             # Not shown
save_fig("precision_recall_vs_threshold_plot")                                              # Not shown
plt.show()
```

绘制精度关于召回率曲线：

```python
# precision_vs_recall
def plot_precision_vs_recall(precisions, recalls):
    plt.plot(recalls, precisions, "b-", linewidth=2)
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.grid(True)

plt.figure(figsize=(8, 6))
plot_precision_vs_recall(precisions, recalls)
plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], "r:")
plt.plot([0.0, recall_90_precision], [0.9, 0.9], "r:")
plt.plot([recall_90_precision], [0.9], "ro")
save_fig("precision_vs_recall_plot")
plt.show()
```

### ROC曲线

ROC_AUC的含义 p97

```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal
    plt.axis([0, 1, 0, 1])                                    # Not shown in the book
    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown
    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown
    plt.grid(True)                                            # Not shown

plt.figure(figsize=(8, 6))                                    # Not shown
plot_roc_curve(fpr, tpr)
fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           # Not shown
plt.plot([fpr_90, fpr_90], [0., recall_90_precision], "r:")   # Not shown
plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], "r:")  # Not shown
plt.plot([fpr_90], [recall_90_precision], "ro")               # Not shown
save_fig("roc_curve_plot")                                    # Not shown
plt.show()
```

## 多类分类器

可以使用朴素贝叶斯、决策树、sgd（p100）等直接实现多类分类

也可以使用多个二分类器实现：

1. ovr策略：对每个类，在全体数据上训练一个判断是否是这个类的分类器。有多少类就有多少个分类器。预测时选择分数最高（决策函数值最大）的分类器对应的类
2. ovo策略：每两个类之间训练一个判断是哪个类分类器，只需要在部分数据上训练即可，但根据排列组合，分类器数量还是会比较多。预测时选择“获胜”最多次的类

事实上，数据集规模较大时ovo策略速度快并且效果好。普遍情况是ovr更好

实现：svm分类器自带多分类，或者选择实现方式再选择内核

```python
from sklearn.svm import SVC  # 根据数据集选择了ovr

svm_clf = SVC()
svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5
svm_clf.predict([some_digit])

# 使用决策函数获取各个类的分数
some_digit_scores = svm_clf.decision_function([some_digit])
some_digit_scores
```

```python
from sklearn.multiclass import OneVsRestClassifier
ovr_clf = OneVsRestClassifier(SVC())
ovr_clf.fit(X_train[:1000], y_train[:1000])
ovr_clf.predict([some_digit])
```

## 多分类误差分析

- 获取混淆矩阵后绘制热力图
- 比较的应该是错误率而不是错误数量
- 对输入进行方所处理能提高准确率

## 多标签分类

含义见p104

使用knn

```python
from sklearn.neighbors import KNeighborsClassifier

y_train_large = (y_train >= 7)
y_train_odd = (y_train % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)
```



## 多输出分类

含义见p105

例子更像是做聚类

```python
noise = np.random.randint(0, 100, (len(X_train), 784))
X_train_mod = X_train + noise
noise = np.random.randint(0, 100, (len(X_test), 784))
X_test_mod = X_test + noise
y_train_mod = X_train
y_test_mod = X_test
some_index = 0
plt.subplot(121); plot_digit(X_test_mod[some_index])
plt.subplot(122); plot_digit(y_test_mod[some_index])
save_fig("noisy_digit_example_plot")
plt.show()

knn_clf.fit(X_train_mod, y_train_mod)
clean_digit = knn_clf.predict([X_test_mod[some_index]])
plot_digit(clean_digit)
save_fig("cleaned_digit_example_plot")
```

