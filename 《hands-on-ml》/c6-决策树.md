## cart分类树

### 基础案例以及可视化

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
# iris
iris.data
X = iris.data[:, 2:] # petal length and width
y = iris.target
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)

from sklearn.tree import export_graphviz
import graphviz
dot_data = export_graphviz(
    tree_clf,  # 分类器
    out_file=None,
    feature_names=iris.feature_names[2:],  # 特征字段名
    class_names=iris.target_names,  # 分类结果名
    rounded=True,  # 边框圆角
    filled=True  # 填充颜色
)
graph = graphviz.Source(dot_data)
graph.render('决策树可视化')
```

### 参数与方法

```python
# 估计类别概率，输出在各个类别的概率
tree_clf.predict_proba([[5, 1.5]])
=========================================
array([[0.        , 0.90740741, 0.09259259]])
```

```python
# 超参数
criterion: 分裂指标（特征的重要性度量）。"entropy", "gini"
splitter：
	# 分裂节点的选择方式。输入”best"，优先选择更重要的特征进行分支(重要性可以通过属性feature_importances_查看)，输入“random"，决策树在⽀支时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。
random_state: 随机数种子。每次切分时随机选择一部分切分特征，从中选最好的。默认为None


min_samples_split: 分裂前节点至少要有的样本数
min_samples_leaf: 叶节点至少要有的样本数
min_impurity_decrease：限制信息增益的大小，信息增益小于设定数值的分枝不会发生。

max_depth: 树的最大深度
max_leaf_nodes: 最大叶节点数量
max_features:
    # 构造时，在每个结点处选取特征的一个子集，并对其中一个特征寻找最佳测试。选取的特征子集中特征的个数通过max_features参数来控制，max_features越小，随机森林中的树就越不相同，但过小（取1时）会导致在划分时无法选择对哪个特征进行测试。而在sklearn中，max_features有以下几种选取方法："auto", "sqrt", "log2", None。auto与sqrt都是取特征总数的开方，log2取特征总数的对数，None则是令max_features直接等于特征总数，而max_features的默认值是"auto"
    
class_weight：
	# 完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是‘是’vs‘否’（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。
```

## cart回归树

只是把分裂指标改为mse。参数基本都一样

## 技巧

决策树对数据集的旋转敏感，可以用pca预处理数据集

决策树对训练数据的小变化敏感

