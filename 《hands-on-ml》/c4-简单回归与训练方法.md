## 线性回归

### 最小二乘法实现

推导、伪逆、复杂度很重要，这里不说原理了

```python
from sklearn.linear_model import LinearRegression

x1 = 2 * np.random.rand(100, 1)
y = 4 + 3*x1 + np.random.randn(100, 1)  # epsilon服从正太分布
lin_reg = LinearRegression()
lin_reg.fit(x1, y)

lin_reg.intercept_  # 截距项
lin_reg.coef_  # 权重
```

## 三种梯度下降

- 特征数量很多时先归一化

### 批量梯度下降：GD

- 只能针对凸优化找全局最优
- 寻找合适的学习率：带 $\epsilon$ 限制的加快网格搜索
- 数据量大时慢，特征量大时快？？

### 随机梯度下降：SGD

- 可以找到全局较优值，但是需要逐步降低学习率

- 使用随机混洗的技巧

- ```python
  from sklearn.linear_model import SGDRegressor
  sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
  sgd_reg.fit(x1, y.ravel())  # 函数要求转换为行向量
  print(sgd_reg.intercept_, sgd_reg.coef_)
  
  max_iter: 最大迭代次数
  tol：epsilon
  penalty：正则化
  eta0：学习率
  shuffle:是否使用随机混选
  random_state:随机数种子
  ```

### 小批量随机梯度下降

## 提前停止

在随机梯度下降迭代轮次中选择最好的

```python
from sklearn.base import clone

poly_scaler = Pipeline([
        ("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
        ("std_scaler", StandardScaler())
    ])

X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)

sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                       penalty=None, learning_rate="constant", eta0=0.0005, random_state=42)

minimum_val_error = float("inf")
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_val, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sgd_reg)
```

## 多项式回归

### PolynomialFeatures转换器

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
x_poly = poly_features.fit_transform(x)  # x_poly是一个一行的array

# param
degree: 生成项次数
interaction_only: 只生成交叉项
include_bias: 是否包含一个常数项
# attri
powers_: 生成的每一项是怎么组合得到的
```

### 使用流水线

```python
from sklearn.pipeline import Pipeline
poly_reg = Pipeline([
    ('poly_features', PolynomialFeatures(degree=2,include_bias=False)),
    ('linear_reg', LinearRegression()),
])
poly_reg.fit(x, y)
y_pred = poly_reg.predict(x)
```

### 利用学习曲线选择使用的次数(imp)

p124



## 岭回归

闭式解法

```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=1, solver="cholesky")
ridge_reg.fit(X, y)
```

sgd法

```python
sgd_reg = SGDRegressor(penalty="l2", max_iter=1000, tol=1e-3)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
```

## lasso回归

特性：把一些没用的系数降为0

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])

# 也可以使用sgdregressor的l1
```

## 弹性网络

加权岭回归与lasso

```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
```

## logistic回归

```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="lbfgs")
log_reg.fit(X, y)
y_proba = log_reg.predict_proba(X_new)

# param
penalty: 正则化，默认l2
C：正则化系数，但是为1/λ
# method
具有predict_prob方法
```

## softmax回归

多类预测的logistic

```python
softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10,random_state=42)

softmax_reg.fit(X, y)
softmax_reg.predict_proba([[5, 2]])
```

